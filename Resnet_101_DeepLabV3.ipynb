{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_101_DeepLabV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT4jAV/i88FpGyT/bf0ice",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimalkumarasamy/Semantic-Segmentation-Suite/blob/master/Resnet_101_DeepLabV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFeZmcz8N4tR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2aa93a93-cf5e-4388-cf04-45770b403fb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noukz-AfOGcg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c2cea0f-875c-4db8-f4c0-d0b81b3c40af"
      },
      "source": [
        "cd '/content/gdrive/My Drive/Spring 2020/Machine Learning/Project/Semantic-Segmentation-Suite'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Spring 2020/Machine Learning/Project/Semantic-Segmentation-Suite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y89zLR4N-HH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ee50f229-cb4a-452b-d39b-231cf90d639f"
      },
      "source": [
        "! pip install Pillow\n",
        "! pip install scipy==1.1.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzIdTLUCOIoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os,time,cv2, sys, math\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import time, datetime\n",
        "import argparse\n",
        "import random\n",
        "import os, sys, csv\n",
        "import subprocess\n",
        "# use 'Agg' on matplotlib so that plots could be generated even without Xserver\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from frontends import resnet_utils\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hGH1ubNZOuJ1",
        "colab": {}
      },
      "source": [
        "def get_label_info(csv_path):\n",
        "    \"\"\"\n",
        "    Retrieve the class names and label values for the selected dataset.\n",
        "    Must be in CSV format!\n",
        "\n",
        "    # Arguments\n",
        "        csv_path: The file path of the class dictionairy\n",
        "        \n",
        "    # Returns\n",
        "        Two lists: one for the class names and the other for the label values\n",
        "    \"\"\"\n",
        "    filename, file_extension = os.path.splitext(csv_path)\n",
        "    if not file_extension == \".csv\":\n",
        "        return ValueError(\"File is not a CSV!\")\n",
        "\n",
        "    class_names = []\n",
        "    label_values = []\n",
        "    with open(csv_path, 'r') as csvfile:\n",
        "        file_reader = csv.reader(csvfile, delimiter=',')\n",
        "        header = next(file_reader)\n",
        "        for row in file_reader:\n",
        "            class_names.append(row[0])\n",
        "            label_values.append([int(row[1]), int(row[2]), int(row[3])])\n",
        "        # print(class_dict)\n",
        "    return class_names, label_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gqd9iEPOYMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30776653-4ecc-46d6-fe80-49dd607455ee"
      },
      "source": [
        "class_names_list, label_values = get_label_info(os.path.join(\"CamVid\", \"class_dict.csv\"))\n",
        "class_names_string = \"\"\n",
        "for class_name in class_names_list:\n",
        "    if not class_name == class_names_list[-1]:\n",
        "        class_names_string = class_names_string + class_name + \", \"\n",
        "    else:\n",
        "        class_names_string = class_names_string + class_name\n",
        "\n",
        "num_classes = len(label_values)\n",
        "print(\"There are\",num_classes,\"semantic classes\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 32 semantic classes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhk-iBfMO5u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess=tf.Session(config=config)\n",
        "net_input = tf.placeholder(tf.float32,shape=[None,None,None,3])\n",
        "net_output = tf.placeholder(tf.float32,shape=[None,None,None,num_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwVB86PjPoOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e914df05-4e75-42f1-8cf8-d0845d93dad8"
      },
      "source": [
        "if os.path.isfile(\"models/resnet_v2_101.ckpt\"):\n",
        "  print(\"Checkpoint file available, picking it up for transfer learning\")\n",
        "else:\n",
        "  print(\"Checkpoint file not available, downloading from the site\")\n",
        "  print(\"Downloading...\")\n",
        "  subprocess.check_output(['wget','http://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz', \"-P\", \"models\"])\n",
        "  try:\n",
        "    subprocess.check_output(['tar', '-xvf', 'models/resnet_v2_101_2017_04_14.tar.gz', \"-C\", \"models\"])\n",
        "    print(\"Decompressing\")\n",
        "    subprocess.check_output(['rm', 'models/resnet_v2_101_2017_04_14.tar.gz'])\n",
        "    print(\"Checkpoint file now available\")\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    pass"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoint file available, picking it up for transfer learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx8wDrzTPnVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = None\n",
        "init_fn = None\n",
        "model_name=\"DeepLabV3\"\n",
        "frontend=\"ResNet101\"\n",
        "is_training=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilSIxU56XHEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_arg_scope(weight_decay=0.0001,\n",
        "                     is_training=True,\n",
        "                     batch_norm_decay=0.997,\n",
        "                     batch_norm_epsilon=1e-5,\n",
        "                     batch_norm_scale=True,\n",
        "                     activation_fn=tf.nn.relu,\n",
        "                     use_batch_norm=True):\n",
        "  \"\"\"Defines the default ResNet arg scope.\n",
        "\n",
        "  TODO(gpapan): The batch-normalization related default values above are\n",
        "    appropriate for use in conjunction with the reference ResNet models\n",
        "    released at https://github.com/KaimingHe/deep-residual-networks. When\n",
        "    training ResNets from scratch, they might need to be tuned.\n",
        "\n",
        "  Args:\n",
        "    weight_decay: The weight decay to use for regularizing the model.\n",
        "    batch_norm_decay: The moving average decay when estimating layer activation\n",
        "      statistics in batch normalization.\n",
        "    batch_norm_epsilon: Small constant to prevent division by zero when\n",
        "      normalizing activations by their variance in batch normalization.\n",
        "    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n",
        "      activations in the batch normalization layer.\n",
        "    activation_fn: The activation function which is used in ResNet.\n",
        "    use_batch_norm: Whether or not to use batch normalization.\n",
        "\n",
        "  Returns:\n",
        "    An `arg_scope` to use for the resnet models.\n",
        "  \"\"\"\n",
        "  batch_norm_params = {\n",
        "      'decay': batch_norm_decay,\n",
        "      'epsilon': batch_norm_epsilon,\n",
        "      'scale': batch_norm_scale,\n",
        "      'updates_collections': None,\n",
        "      'is_training': is_training,\n",
        "      'fused': True,  # Use fused batch norm if possible.\n",
        "  }\n",
        "\n",
        "  with slim.arg_scope(\n",
        "      [slim.conv2d],\n",
        "      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
        "      weights_initializer=slim.variance_scaling_initializer(),\n",
        "      activation_fn=activation_fn,\n",
        "      normalizer_fn=slim.batch_norm if use_batch_norm else None,\n",
        "      normalizer_params=batch_norm_params):\n",
        "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
        "      # The following implies padding='SAME' for pool1, which makes feature\n",
        "      # alignment easier for dense prediction tasks. This is also used in\n",
        "      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n",
        "      # code of 'Deep Residual Learning for Image Recognition' uses\n",
        "      # padding='VALID' for pool1. You can switch to that choice by setting\n",
        "      # slim.arg_scope([slim.max_pool2d], padding='VALID').\n",
        "      with slim.arg_scope([slim.max_pool2d], padding='SAME') as arg_sc:\n",
        "        return arg_sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0__6UKKCXTM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2_block(scope, base_depth, num_units, stride):\n",
        "  \"\"\"Helper function for creating a resnet_v2 bottleneck block.\n",
        "\n",
        "  Args:\n",
        "    scope: The scope of the block.\n",
        "    base_depth: The depth of the bottleneck layer for each unit.\n",
        "    num_units: The number of units in the block.\n",
        "    stride: The stride of the block, implemented as a stride in the last unit.\n",
        "      All other units have stride=1.\n",
        "\n",
        "  Returns:\n",
        "    A resnet_v2 bottleneck block.\n",
        "  \"\"\"\n",
        "  ##Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args']))\n",
        "  return resnet_utils.Block(scope, bottleneck, [{\n",
        "      'depth': base_depth * 4,\n",
        "      'depth_bottleneck': base_depth,\n",
        "      'stride': 1\n",
        "  }] * (num_units - 1) + [{\n",
        "      'depth': base_depth * 4,\n",
        "      'depth_bottleneck': base_depth,\n",
        "      'stride': stride\n",
        "  }])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJdrwJIqapzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# slim = tf.contrib.slim\n",
        "@slim.add_arg_scope\n",
        "def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,outputs_collections=None, scope=None):\n",
        "  \"\"\"Bottleneck residual unit variant with BN before convolutions.\n",
        "\n",
        "  This is the full preactivation residual unit variant proposed in [2]. See\n",
        "  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n",
        "  variant which has an extra bottleneck layer.\n",
        "\n",
        "  When putting together two consecutive ResNet blocks that use this unit, one\n",
        "  should use stride = 2 in the last unit of the first block.\n",
        "\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, height, width, channels].\n",
        "    depth: The depth of the ResNet unit output.\n",
        "    depth_bottleneck: The depth of the bottleneck layers.\n",
        "    stride: The ResNet unit's stride. Determines the amount of downsampling of\n",
        "      the units output compared to its input.\n",
        "    rate: An integer, rate for atrous convolution.\n",
        "    outputs_collections: Collection to add the ResNet unit output.\n",
        "    scope: Optional variable_scope.\n",
        "\n",
        "  Returns:\n",
        "    The ResNet unit's output.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:\n",
        "    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
        "    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')\n",
        "    if depth == depth_in:\n",
        "      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n",
        "    else:\n",
        "      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n",
        "                             normalizer_fn=None, activation_fn=None,\n",
        "                             scope='shortcut')\n",
        "\n",
        "    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n",
        "                           scope='conv1')\n",
        "    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n",
        "                                        rate=rate, scope='conv2')\n",
        "    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n",
        "                           normalizer_fn=None, activation_fn=None,\n",
        "                           scope='conv3')\n",
        "\n",
        "    output = shortcut + residual\n",
        "\n",
        "    return slim.utils.collect_named_outputs(outputs_collections,\n",
        "                                            sc.name,\n",
        "                                            output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HydprGTvfqv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2(inputs,\n",
        "              blocks,\n",
        "              num_classes=None,\n",
        "              is_training=True,\n",
        "              global_pool=True,\n",
        "              output_stride=None,\n",
        "              include_root_block=True,\n",
        "              spatial_squeeze=True,\n",
        "              reuse=True,\n",
        "              scope=None):\n",
        "  \"\"\"Generator for v2 (preactivation) ResNet models.\n",
        "\n",
        "  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n",
        "  methods for specific model instantiations, obtained by selecting different\n",
        "  block instantiations that produce ResNets of various depths.\n",
        "\n",
        "  Training for image classification on Imagenet is usually done with [224, 224]\n",
        "  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n",
        "  block for the ResNets defined in [1] that have nominal stride equal to 32.\n",
        "  However, for dense prediction tasks we advise that one uses inputs with\n",
        "  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n",
        "  this case the feature maps at the ResNet output will have spatial shape\n",
        "  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n",
        "  and corners exactly aligned with the input image corners, which greatly\n",
        "  facilitates alignment of the features to the image. Using as input [225, 225]\n",
        "  images results in [8, 8] feature maps at the output of the last ResNet block.\n",
        "\n",
        "  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n",
        "  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n",
        "  have nominal stride equal to 32 and a good choice in FCN mode is to use\n",
        "  output_stride=16 in order to increase the density of the computed features at\n",
        "  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n",
        "\n",
        "  Args:\n",
        "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
        "    blocks: A list of length equal to the number of ResNet blocks. Each element\n",
        "      is a resnet_utils.Block object describing the units in the block.\n",
        "    num_classes: Number of predicted classes for classification tasks.\n",
        "      If 0 or None, we return the features before the logit layer.\n",
        "    is_training: whether batch_norm layers are in training mode.\n",
        "    global_pool: If True, we perform global average pooling before computing the\n",
        "      logits. Set to True for image classification, False for dense prediction.\n",
        "    output_stride: If None, then the output will be computed at the nominal\n",
        "      network stride. If output_stride is not None, it specifies the requested\n",
        "      ratio of input to output spatial resolution.\n",
        "    include_root_block: If True, include the initial convolution followed by\n",
        "      max-pooling, if False excludes it. If excluded, `inputs` should be the\n",
        "      results of an activation-less convolution.\n",
        "    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n",
        "        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n",
        "        To use this parameter, the input images must be smaller than 300x300\n",
        "        pixels, in which case the output logit layer does not contain spatial\n",
        "        information and can be removed.\n",
        "    reuse: whether or not the network and its variables should be reused. To be\n",
        "      able to reuse 'scope' must be given.\n",
        "    scope: Optional variable_scope.\n",
        "\n",
        "\n",
        "  Returns:\n",
        "    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n",
        "      If global_pool is False, then height_out and width_out are reduced by a\n",
        "      factor of output_stride compared to the respective height_in and width_in,\n",
        "      else both height_out and width_out equal one. If num_classes is 0 or None,\n",
        "      then net is the output of the last ResNet block, potentially after global\n",
        "      average pooling. If num_classes is a non-zero integer, net contains the\n",
        "      pre-softmax activations.\n",
        "    end_points: A dictionary from components of the network to the corresponding\n",
        "      activation.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the target output_stride is not valid.\n",
        "  \"\"\"\n",
        "  with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:\n",
        "    end_points_collection = sc.original_name_scope + '_end_points'\n",
        "    with slim.arg_scope([slim.conv2d, bottleneck,\n",
        "                         resnet_utils.stack_blocks_dense],\n",
        "                        outputs_collections=end_points_collection):\n",
        "      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
        "        net = inputs\n",
        "        if include_root_block:\n",
        "          if output_stride is not None:\n",
        "            if output_stride % 4 != 0:\n",
        "              raise ValueError('The output_stride needs to be a multiple of 4.')\n",
        "            output_stride /= 4\n",
        "          # We do not include batch normalization or activation functions in\n",
        "          # conv1 because the first ResNet unit will perform these. Cf.\n",
        "          # Appendix of [2].\n",
        "          with slim.arg_scope([slim.conv2d],\n",
        "                              activation_fn=None, normalizer_fn=None):\n",
        "            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
        "          net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
        "\n",
        "          net = slim.utils.collect_named_outputs(end_points_collection, 'pool2', net)\n",
        "        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n",
        "        # This is needed because the pre-activation variant does not have batch\n",
        "        # normalization or activation functions in the residual unit output. See\n",
        "        # Appendix of [2].\n",
        "        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')\n",
        "        # Convert end_points_collection into a dictionary of end_points.\n",
        "        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
        "\n",
        "        end_points['pool3'] = end_points[scope + '/block1']\n",
        "        end_points['pool4'] = end_points[scope + '/block2']\n",
        "        end_points['pool5'] = net\n",
        "        return net, end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGW9U6Luf_ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_v2_101(inputs,\n",
        "                  num_classes=None,\n",
        "                  is_training=True,\n",
        "                  global_pool=True,\n",
        "                  output_stride=None,\n",
        "                  spatial_squeeze=True,\n",
        "                  reuse=None,\n",
        "                  scope='resnet_v2_101'):\n",
        "  \"\"\"ResNet-101 model of [1]. See resnet_v2() for arg and return description.\"\"\"\n",
        "  blocks = [\n",
        "      resnet_v2_block('block1', base_depth=64, num_units=3, stride=2),\n",
        "      resnet_v2_block('block2', base_depth=128, num_units=4, stride=2),\n",
        "      resnet_v2_block('block3', base_depth=256, num_units=23, stride=2),\n",
        "      resnet_v2_block('block4', base_depth=512, num_units=3, stride=1),\n",
        "  ]\n",
        "  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n",
        "                   global_pool=global_pool, output_stride=output_stride,\n",
        "                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n",
        "                   reuse=reuse, scope=scope)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiUViW7tVFdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preset_model='DeepLabV3'\n",
        "frontend=\"Res101\"\n",
        "weight_decay=1e-5\n",
        "is_training=True\n",
        "pretrained_dir=\"models\"\n",
        "frontend_scope='resnet_v2_101'\n",
        "inputs=net_input\n",
        "with slim.arg_scope(resnet_arg_scope()):\n",
        "  logits, end_points = resnet_v2_101(net_input, is_training=is_training, scope='resnet_v2_101')\n",
        "  init_fn = slim.assign_from_checkpoint_fn(model_path=os.path.join(pretrained_dir, 'resnet_v2_101.ckpt'), var_list=slim.get_model_variables('resnet_v2_101'), ignore_missing_vars=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjF0I3mWl2o1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AtrousSpatialPyramidPoolingModule(inputs, depth=256):\n",
        "    \"\"\"\n",
        "\n",
        "    ASPP consists of (a) one 1×1 convolution and three 3×3 convolutions with rates = (6, 12, 18) when output stride = 16\n",
        "    (all with 256 filters and batch normalization), and (b) the image-level features as described in the paper\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    feature_map_size = tf.shape(inputs)\n",
        "\n",
        "    # Global average pooling\n",
        "    image_features = tf.reduce_mean(inputs, [1, 2], keep_dims=True)\n",
        "\n",
        "    image_features = slim.conv2d(image_features, depth, [1, 1], activation_fn=None)\n",
        "    image_features = tf.image.resize_bilinear(image_features, (feature_map_size[1], feature_map_size[2]))\n",
        "\n",
        "    atrous_pool_block_1 = slim.conv2d(inputs, depth, [1, 1], activation_fn=None)\n",
        "\n",
        "    atrous_pool_block_6 = slim.conv2d(inputs, depth, [3, 3], rate=6, activation_fn=None)\n",
        "\n",
        "    atrous_pool_block_12 = slim.conv2d(inputs, depth, [3, 3], rate=12, activation_fn=None)\n",
        "\n",
        "    atrous_pool_block_18 = slim.conv2d(inputs, depth, [3, 3], rate=18, activation_fn=None)\n",
        "\n",
        "    net = tf.concat((image_features, atrous_pool_block_1, atrous_pool_block_6, atrous_pool_block_12, atrous_pool_block_18), axis=3)\n",
        "    net = slim.conv2d(net, depth, [1, 1], scope=\"conv_1x1_output\", activation_fn=None)\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sleZ84glWLhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_size = tf.shape(inputs)[1:3]\n",
        "net = AtrousSpatialPyramidPoolingModule(end_points['pool4'])\n",
        "# net = Upsampling(net, label_size)\n",
        "net=tf.image.resize_bilinear(net, size=label_size)\n",
        "net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, scope='logits')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ZH3Z6HWA6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4a5dabf-55b8-456a-8276-bde22cf3b571"
      },
      "source": [
        "net"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'logits/BiasAdd:0' shape=(?, ?, ?, 32) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfamzkVgmlP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7302c507-633b-4ff4-8729-571e47eece12"
      },
      "source": [
        "logits"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'resnet_v2_101/postnorm/Relu:0' shape=(?, ?, ?, 2048) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AixaOsNgmp3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}